# -*- coding: utf-8 -*-
"""Problem2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rVive_BZRdpvFDKznoPA7crrG306czPJ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import time
import matplotlib.pyplot as plt

# Define dataset transforms (Normalization for CIFAR-10)
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]
])

# Load training and test data
batch_size = 128
trainset = torchvision.datasets.CIFAR10(root="./data", train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root="./data", train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)

# Define class labels
classes = ("plane", "car", "bird", "cat", "deer", "dog", "frog", "horse", "ship", "truck")

# Define CNN Model for Experiment 1
# ReLU -> Tanh -> Sigmoid -> ReLU
class CNN_Exp1(nn.Module):
    def __init__(self):
        super(CNN_Exp1, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)

        self.pool = nn.MaxPool2d(2, 2)

        self.fc1 = nn.Linear(128 * 2 * 2, 256)
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.pool(nn.ReLU()(self.conv1(x)))
        x = self.pool(nn.Tanh()(self.conv2(x)))
        x = self.pool(nn.Sigmoid()(self.conv3(x)))
        x = self.pool(nn.ReLU()(self.conv4(x)))

        x = x.view(-1, 128 * 2 * 2)
        x = nn.ReLU()(self.fc1(x))
        x = self.fc2(x)
        return x

# Define CNN Model for Experiment 2
#ReLU -> Sigmoid -> Sigmoid -> Tanh
class CNN_Exp2(nn.Module):
    def __init__(self):
        super(CNN_Exp2, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)

        self.pool = nn.MaxPool2d(2, 2)

        self.fc1 = nn.Linear(128 * 2 * 2, 256)
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.pool(nn.ReLU()(self.conv1(x)))
        x = self.pool(nn.Sigmoid()(self.conv2(x)))
        x = self.pool(nn.Sigmoid()(self.conv3(x)))
        x = self.pool(nn.Tanh()(self.conv4(x)))

        x = x.view(-1, 128 * 2 * 2)
        x = nn.Tanh()(self.fc1(x))
        x = self.fc2(x)
        return x

# Define training function
def train_model(model, trainloader, criterion, optimizer, max_epochs=50):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    epoch_times = []
    epoch_errors = []

    for epoch in range(max_epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        start_time = time.time()

        for inputs, labels in trainloader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        epoch_time = time.time() - start_time
        epoch_times.append(epoch_time)

        train_error = 100 * (1 - correct / total)
        epoch_errors.append(train_error)
        avg_loss = running_loss / total

        print(f"Epoch {epoch+1}: Loss={avg_loss:.3f} | Training Error={train_error:.2f}% | Time={epoch_time:.2f} sec")

        if train_error <= 25:
            print(f"Stopping early at epoch {epoch+1} (Training error â‰¤ 25%)")
            break

    return epoch_times, epoch_errors



# Initialize models, loss function, and optimizers
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
criterion = nn.CrossEntropyLoss()

# CNN 1 Training
print("\nTraining CNN Experiment 1 (ReLU-Tanh-Sigmoid-ReLU)")
model_exp1 = CNN_Exp1().to(device)
optimizer_exp1 = optim.SGD(model_exp1.parameters(), lr=0.01, momentum=0.9)
time_exp1, error_exp1 = train_model(model_exp1, trainloader, criterion, optimizer_exp1) # Capture both return values

#Train the second CNN
# Experiment 2
print("\nTraining CNN Experiment 2 (ReLU-Sigmoid-Sigmoid-Tanh)")
model_exp2 = CNN_Exp2().to(device)
optimizer_exp2 = optim.SGD(model_exp2.parameters(), lr=0.01, momentum=0.9)
time_exp2, error_exp2 = train_model(model_exp2, trainloader, criterion, optimizer_exp2) # Capture both return values

# Plot Training Error vs. Epochs
plt.figure(figsize=(8, 6))
plt.plot(range(1, len(error_exp1) + 1), error_exp1, label="Exp1: ReLU-Tanh-Sigmoid-ReLU", marker="o")
plt.plot(range(1, len(error_exp2) + 1), error_exp2, label="Exp2: ReLU-Sigmoid-Sigmoid-Tanh", marker="s")
plt.xlabel("Epochs")
plt.ylabel("Training Error (%)")
plt.title("Training Error vs. Epochs for Different Activation Functions")
plt.legend()
plt.grid(True)
plt.show()

